{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e1992f",
   "metadata": {},
   "source": [
    "# DigRecog-Q1.py (Baseline CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28->26->13\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 13->11->5\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate train accuracy for this batch\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct_train += pred.eq(target).sum().item()\n",
    "        total_train += target.size(0)\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    train_accuracies.append(correct_train / total_train)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    test_loss = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct_test += pred.eq(target).sum().item()\n",
    "            total_test += target.size(0)\n",
    "    \n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracies.append(correct_test / total_test)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Test Loss={test_losses[-1]:.4f}, '\n",
    "          f'Train Acc={train_accuracies[-1]*100:.2f}%, Test Acc={test_accuracies[-1]*100:.2f}%')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot([a * 100 for a in train_accuracies], label='Train Accuracy', color='blue')\n",
    "plt.plot([a * 100 for a in test_accuracies], label='Test Accuracy', color='green')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9091b8a",
   "metadata": {},
   "source": [
    "# DigRecogDeeperCNN-Q2.py (Variant A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations for MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Variant A: Deeper CNN Model\n",
    "class CNNDeeper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDeeper, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)     # Output: (32, 26, 26)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)    # Output: (64, 24, 24)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)   # Output: (128, 22, 22)\n",
    "        self.pool = nn.MaxPool2d(2, 2)                   # Halves dimensions\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Use dummy input to calculate flattened size\n",
    "        dummy_input = torch.zeros(1, 1, 28, 28)\n",
    "        dummy_out = self._get_conv_output(dummy_input)\n",
    "        self.fc1 = nn.Linear(dummy_out, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def _get_conv_output(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (32, 13, 13)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (64, 5, 5)\n",
    "        x = F.relu(self.conv3(x))             # (128, 3, 3) --> depends on kernel/pool sizes\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = CNNDeeper().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    test_losses.append(avg_test_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "          f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plotting accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title(\"Loss Curve (Deeper CNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label='Test Accuracy', color='green')\n",
    "plt.title(\"Accuracy Curve (Deeper CNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796e547",
   "metadata": {},
   "source": [
    "# DigRecogDifferentKernels-Q2.py (Variant B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caff737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations for MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "class CNN_KernelSizes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_KernelSizes, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28→28→14\n",
    "        x = F.relu(self.conv2(x))             # 14→14\n",
    "        x = x.view(-1, 32 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = CNN_KernelSizes().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    test_losses.append(avg_test_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "          f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plotting accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title(\"Loss Curve (Different Kernels CNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label='Test Accuracy', color='green')\n",
    "plt.title(\"Accuracy Curve (Different Kernels CNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32683f79",
   "metadata": {},
   "source": [
    "# DigRecogLeakyRELU+BatchNorm-Q2.py (Variant C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations for MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "class CNN_Leaky_BatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Leaky_BatchNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.act(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = CNN_Leaky_BatchNorm().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    test_losses.append(avg_test_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "          f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plotting accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title(\"Loss Curve (LeakyRELU+BatchNorm CNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label='Test Accuracy', color='green')\n",
    "plt.title(\"Accuracy Curve (LeakyRELU+BatchNorm CNN)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49749d99",
   "metadata": {},
   "source": [
    "# DigRecogExperiment-Q3.py (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ff4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and transforms\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# Model definition\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Use dummy input to compute flattened feature size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 28, 28)\n",
    "            dummy_output = self._forward_features(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> [batch, 32, 13, 13]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> [batch, 64, 5, 5]\n",
    "        x = F.relu(self.conv3(x))             # -> [batch, 128, 3, 3]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)             # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Optional weight init\n",
    "def init_weights_he(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Train + Eval function\n",
    "def train_and_eval(config):\n",
    "    model = CustomCNN(dropout_rate=config['dropout']).to(device)\n",
    "    if config['weight_init'] == 'he':\n",
    "        model.apply(init_weights_he)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = config['optimizer'](model.parameters(), lr=config['lr'])\n",
    "\n",
    "    train_loss_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        test_acc_list.append(accuracy)\n",
    "\n",
    "    return train_loss_list, test_acc_list\n",
    "\n",
    "# Configs to try\n",
    "configs = [\n",
    "    {'lr': 0.001, 'optimizer': optim.Adam, 'batch_size': 64, 'dropout': 0.5, 'weight_init': 'he'},\n",
    "    {'lr': 0.01, 'optimizer': optim.Adam, 'batch_size': 64, 'dropout': 0.5, 'weight_init': 'he'},\n",
    "    {'lr': 0.001, 'optimizer': optim.SGD, 'batch_size': 64, 'dropout': 0.5, 'weight_init': 'he'},\n",
    "    {'lr': 0.001, 'optimizer': optim.Adam, 'batch_size': 128, 'dropout': 0.3, 'weight_init': 'he'},\n",
    "    {'lr': 0.001, 'optimizer': optim.Adam, 'batch_size': 32, 'dropout': 0.5, 'weight_init': 'he'},\n",
    "    {'lr': 0.001, 'optimizer': optim.Adam, 'batch_size': 64, 'dropout': 0.7, 'weight_init': 'he'},\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "results = []\n",
    "for idx, config in enumerate(configs):\n",
    "    print(f\"\\nRunning Config {idx + 1}/{len(configs)}: {config}\")\n",
    "    train_losses, test_accuracies = train_and_eval(config)\n",
    "    results.append((config, train_losses, test_accuracies))\n",
    "    print(f\"Final Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (config, _, accs) in enumerate(results):\n",
    "    plt.plot(accs, label=f\"Cfg {i+1}: FinalAcc={accs[-1]:.1f}%\")\n",
    "plt.title(\"Test Accuracy across Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f4647",
   "metadata": {},
   "source": [
    "# DigRecogFail1 - High lr.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28->26->13\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 13->11->5\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "train_losses, test_losses, accuracies = [], [], []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    accuracies.append(correct / len(test_loader.dataset))\n",
    "    print(f'Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Test Loss={test_losses[-1]:.4f}, Accuracy={accuracies[-1]*100:.2f}%')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accuracies, label='Accuracy', color='green')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508aa2c9",
   "metadata": {},
   "source": [
    "# DigRecogFail2 - BrokenCNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "class BrokenCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrokenCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)  # output: 32 x 26 x 26\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3) # output: 64 x 24 x 24\n",
    "        self.fc1 = nn.Linear(64 * 24 * 24, 128)  # flattened size matches output of conv2\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Identity()  # no dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # no activation\n",
    "        x = self.conv2(x)  # no activation\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.fc1(x)            # no activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = BrokenCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "train_losses, test_losses, accuracies = [], [], []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    accuracies.append(correct / len(test_loader.dataset))\n",
    "    print(f'Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Test Loss={test_losses[-1]:.4f}, Accuracy={accuracies[-1]*100:.2f}%')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accuracies, label='Accuracy', color='green')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9128b12",
   "metadata": {},
   "source": [
    "# DigRecogFail3 - no normalization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82578c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transform WITHOUT normalization to break performance\n",
    "transform = transforms.ToTensor()  # <-- Removed Normalize step\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28->26->13\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 13->11->5\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "for epoch in range(4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate train accuracy for this batch\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct_train += pred.eq(target).sum().item()\n",
    "        total_train += target.size(0)\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    train_accuracies.append(correct_train / total_train)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    test_loss = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct_test += pred.eq(target).sum().item()\n",
    "            total_test += target.size(0)\n",
    "\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracies.append(correct_test / total_test)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Test Loss={test_losses[-1]:.4f}, '\n",
    "          f'Train Acc={train_accuracies[-1]*100:.2f}%, Test Acc={test_accuracies[-1]*100:.2f}%')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss Curve (No Normalization)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot([a * 100 for a in train_accuracies], label='Train Accuracy', color='blue')\n",
    "plt.plot([a * 100 for a in test_accuracies], label='Test Accuracy', color='green')\n",
    "plt.title('Accuracy Curve (No Normalization)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
